{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to set up functions and modules for data preprocessing, options are given to choose e.g., threshold for filtering, filling strategy for NAs. \n",
    "\n",
    "Generally speaking, the preprocessing steps contain filtering out columns with NAs; removing those with single value; separating time series, numerical and categorical data; performing outlier removal; shifting, log transformation and standardization of numerical data; encoding of categorical data. In this notebook, the time series and the categorical data are not standardized. See the other notebooks for the standardization of categorical and time series data. \n",
    "\n",
    "The notebook is divided into the following steps:\n",
    "\n",
    "1. Filter out columns that have too many NAs, users can specify the percentage, functions to impute missing values are also given. Filter out also columns that are constant, NA is optionally considered as a meaningful value. \n",
    "  **example output: xtrain_narm, xtrain_noconst**\n",
    "    \n",
    "2. Numerical columns with very few values are categorized into categorical features. For the rest numerical features, modules are given to do log transform, all integer-valued columns including columns that contain integer values and NA (as float) are first shifted to be positive-valued (with min = 1) and then log transformed if criteria are met according to the normaltest function. The untouched columns are either decimal-valued, or multi-modal distributed\n",
    "  **example output: nxtrain, nxtrain_shifted, nxtrain_log**\n",
    "\n",
    "3. Time series data, most of these columns have a lot of NAs. These features are converted into datetime format so that further derivation can be easily constructed.\n",
    "  **example output: txtrain, txtrain_na_rm**\n",
    "\n",
    "4. Categorical features fall into three types:\n",
    "   (1) object-type columns with very few values: these are converted to feature matrices using DictVectorizer\n",
    "   (2) integer-categorical columns with very few values: these can be converted into categorical with the provided function, yet, the ordinal information will be lost\n",
    "   (3) object-type columns with a lot of values: these are unchanged so far, may contain states, text, etc.\n",
    "  **example output: cat_sparse, ordinal_sparse**\n",
    "\n",
    "Note that now training and testing data are processed together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "#import seaborn as sns\n",
    "%matplotlib inline\n",
    "import requests\n",
    "#from pattern import web\n",
    "import operator\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn import cross_validation\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from scipy import stats\n",
    "from sklearn.base import TransformerMixin\n",
    "from datetime import datetime as dt\n",
    "from math import isnan\n",
    "from numpy import ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack,csr_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liw5\\AppData\\Local\\Continuum\\Anaconda3new\\lib\\site-packages\\pandas\\io\\parsers.py:1170: DtypeWarning: Columns (8,9,10,11,12,43,157,196,214,225,228,229,231,235,238) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    }
   ],
   "source": [
    "na_values = ['[]','',-1]\n",
    "train = pd.read_csv('../input/train.csv',na_values = na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liw5\\AppData\\Local\\Continuum\\Anaconda3new\\lib\\site-packages\\pandas\\io\\parsers.py:1170: DtypeWarning: Columns (8,9,10,11,12,43,157,167,177,196,214,225,228,229,231,235,238) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = self._reader.read(nrows)\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('../input/test.csv',na_values = na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mixed_col_num = [8,9,10,11,12,43,157,196,214,225,228,229,231,235,238]\n",
    "mixed_cols = [cols[i] for i in mixed_col_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain = train.drop(['target','ID'],axis = 1)\n",
    "indices_train = xtrain.index #to be used later for separating train and test data parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtest = test.drop(['ID'],axis = 1)\n",
    "#shift the index of test data before concat\n",
    "indices_test = xtest.index\n",
    "indices_test = indices_test + xtrain.shape[0] \n",
    "xtest.index = indices_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#release memory\n",
    "%xdel train\n",
    "%xdel test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtotal = pd.concat([xtrain, xtest])\n",
    "indices_total = xtotal.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145231, 1932)\n",
      "(290463, 1932)\n"
     ]
    }
   ],
   "source": [
    "%xdel xtest\n",
    "print (xtrain.shape)\n",
    "print (xtotal.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pre-processing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropNA = True       #drop columns with NAs > than dropNAThresh. This will also change the behavior of dropconstant_col.\n",
    "dropNAThresh = 0.98 #percentage\n",
    "\n",
    "separate_cat_num_thresh = 1 #treat those numerical variables with less than th unique values as categorical\n",
    "\n",
    "dropNA_time= True\n",
    "dropNAThresh_time = 0.75 #for time and dates\n",
    "\n",
    "dropNA_cat = False #perform additional nan dropping to categorical data\n",
    "dropNAThresh_cat = 0.75 #for categorical data\n",
    "\n",
    "outlierThresh = 5\n",
    "outlierUniqueThresh = 20 #if number of unique values in a column is less than this threshold, do not apply outlier removal.\n",
    "remove_personal_income_outlier = False #do not remove outlier in personal income\n",
    "\n",
    "fillNAStrategy_numeric= 'mean' # 'mode', 'median', 'mean', or number\n",
    "fillNAStrategy_time = 'median'#strategy for filling missing values in the original time variables\n",
    "fillNAStrategy_time_derived = 'mode'#strategy for filling missing values in the derived time variables\n",
    "fillNAStrategy_cat_num = 'mode' #strategy for filling missing values in categorical variables which are numeric type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1.Filter columns based on number of missing values and unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameFilter(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Filter feature columns based on nan values and/or constant values\n",
    "        To transform, target must be specified as either \"nan\" or \"constant\"\n",
    "        \n",
    "        \"\"\"\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None, target = 'nan',thresh = 0.98):\n",
    "        \"\"\"\n",
    "        X is the feature dataframe, target can be \"nan\" or \"constant\", \n",
    "        thresh is the fraction of nans or the constant value that justifies the \n",
    "        feature column to be filtered\n",
    "        \"\"\"\n",
    "        if target != 'nan' and target != 'constant':\n",
    "            raise KeyError('Invalid target, valide targets are nan and constant')\n",
    "        elif target == 'nan':\n",
    "            num_nan = X.isnull().sum()\n",
    "            perc_nan = num_nan.apply(lambda x: float(x)/(X.shape[0]))\n",
    "            return X.drop([x for x in perc_nan.index if perc_nan[x] > thresh],axis = 1)\n",
    "        else:\n",
    "            const_fraction = X.apply(lambda x: x.value_counts(normalize=True,dropna = False).values[0])\n",
    "            kept_cols = const_fraction[const_fraction < thresh].index.tolist()\n",
    "            return X[kept_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dffilter = DataFrameFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dropNA:\n",
    "    xtotal = dffilter.transform(xtotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtotal = dffilter.transform(xtotal,target = 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290463, 1722)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtotal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Missing value imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this step can also be done with the ML models, e.g., for example, xgboost has option to impute the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with median,mean,mode or a certain number e.g., -999.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None,strategy = 'median'):\n",
    "\n",
    "        fill_list = []\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == np.dtype('O') or strategy == 'mode':\n",
    "                fill_list.append(X[col].value_counts().index[0])\n",
    "            else:\n",
    "                if strategy == 'median':\n",
    "                    fill_list.append(X[col].median())\n",
    "                elif strategy == 'mean':\n",
    "                    fill_list.append(X[col].mean())\n",
    "                elif type(strategy) in [int,float]:\n",
    "                    fill_list.append(float(strategy))\n",
    "                else:\n",
    "                    print ('unknown method of filling, leave nan')\n",
    "                    fill_list.append(np.nan)\n",
    "        \n",
    "        self.fill =pd.Series(fill_list,index = X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None,strategy = 'median'):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Remove Outliers using median-absolute-deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliers_mad(data, thresh=3.5, outlier_uq_thresh=20):\n",
    "    '''\n",
    "    detect outliers using median-absolute-deviation (MAD)\n",
    "    input: data:   dataframe or series\n",
    "           thresh: threshold\n",
    "           outlier_uq_thresh: if number of unique values in a column is less than this threshold, \n",
    "                   outlier removal will not be applied to that column. \n",
    "    output:num_outliers\n",
    "           num_outliers: count of outliers\n",
    "    '''\n",
    "    abs_deviation = (data - data.median()).abs()\n",
    "    median_abs_deviation = abs_deviation.median()\n",
    "    zero_mad = median_abs_deviation==0\n",
    "    if len(data.shape)==1:\n",
    "        if zero_mad:\n",
    "            median_abs_deviation = 1\n",
    "    else:\n",
    "        median_abs_deviation[zero_mad] = 1  \n",
    "   \n",
    "    modified_z_score = (0.6745 * abs_deviation / median_abs_deviation)>thresh\n",
    "    \n",
    "    if len(data.shape)==1:\n",
    "        if zero_mad:\n",
    "            modified_z_score.loc[:]=False\n",
    "    else:\n",
    "        modified_z_score[zero_mad[zero_mad].index] = False \n",
    "    \n",
    "    #check number of unique values\n",
    "    if len(data.shape)==1:\n",
    "        num_unique = data.nunique(dropna=True)\n",
    "    else:\n",
    "        num_unique = data.apply(lambda x: x.nunique(dropna=True),axis = 0)\n",
    "        \n",
    "    if len(data.shape)==1:\n",
    "        if num_unique < outlier_uq_thresh:\n",
    "            modified_z_score.loc[:]=False\n",
    "    else:\n",
    "        modified_z_score[num_unique[num_unique < outlier_uq_thresh].index] = False\n",
    "        \n",
    "    data[modified_z_score]=np.nan\n",
    "\n",
    "    return data, modified_z_score.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Time series data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_cols = set(['VAR_0073','VAR_0075','VAR_0156','VAR_0157',\n",
    "           'VAR_0158','VAR_0159','VAR_0166','VAR_0167','VAR_0168','VAR_0169',\n",
    "           'VAR_0176','VAR_0177','VAR_0178','VAR_0179','VAR_0204','VAR_0217','VAR_0314','VAR_0531']).intersection(set(xtotal.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_cols = list(time_cols)\n",
    "time_cols.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TimeSeriesTransform(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        convert time series columns from string-valued to datetime obj\n",
    "        \"\"\"\n",
    "    \n",
    "    def fit(self,X, y= None):\n",
    "            return self\n",
    "    \n",
    "    def convertdatetime(self,string,form = '%d%b%y:%H:%M:%S'):\n",
    "        \"\"\"\n",
    "        convert datetime string to datetime obj\n",
    "        inputs: string, form: the format of the string\n",
    "        output: the datetime obj\n",
    "        \"\"\"\n",
    "        if type(string) == str or type(string) == np.string_:\n",
    "            return pd.to_datetime(string,format = form)\n",
    "    \n",
    "    def transform(self,X, y=None):\n",
    "        \"\"\"\n",
    "        X is the sub-dataframe of time serie columns,\n",
    "        the current set-up relies on the correct numbering of the columns, namely, \n",
    "        there are 15 ts columns in total, and the last two are 0314 and 0531\n",
    "        \"\"\"\n",
    "        cols = X.columns.tolist()\n",
    "        X1 = X.ix[:,:13].applymap(self.convertdatetime)\n",
    "        X2 = X['VAR_0314'].copy()\n",
    "        X3 = X['VAR_0531'].copy()\n",
    "        notnull_0314 = X2[X2.notnull()].index.tolist()\n",
    "        notnull_0531 = X3[X3.notnull()].index.tolist()\n",
    "        X2[notnull_0314] = X.ix[notnull_0314,'VAR_0314'].astype(int).astype(str).apply(self.convertdatetime,form = '%Y')\n",
    "        X3[notnull_0531] = X.ix[notnull_0531,'VAR_0531'].astype(int).astype(str).apply(self.convertdatetime,form = '%Y%m')\n",
    "        X_tmp = pd.concat([X2,X3],axis = 1)\n",
    "        return pd.concat([X1,X_tmp],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tstransform = TimeSeriesTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290463, 15)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txtotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txtotal_raw = xtotal[time_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "txtotal = tstransform.transform(txtotal_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Consider dropping columns that have more than 50% NAs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative features such as the difference between each two time series columns can be constructed directly by substraction of one column from the other, these can be generated later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDerivatives(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Adding derived time series columns (duration, year, month, day, weekday, epoch time) \n",
    "        from string-valued to datetime obj\n",
    "        \"\"\"\n",
    "    \n",
    "    def fit(self,X, y= None):\n",
    "            return self\n",
    "        \n",
    "    def transform(self,X, y=None, option=1):\n",
    "        \"\"\"\n",
    "        X is the sub-dataframe of time serie columns\n",
    "        option = 1: adding derived variables\n",
    "        option = 0: only convert the original datetime to days since 1970-1-1\n",
    "        It is assumed that the column names are in this format :\"VAR_xxxx\", \n",
    "        where \"xxxx\" represents the variable number.\n",
    "        \"\"\"\n",
    "        cols = X.columns\n",
    "        if option==1:\n",
    "            cols_derived = []\n",
    "            #duration variables, in days\n",
    "            for i in range(len(cols)-1):\n",
    "                for j in range(i+1, len(cols)):\n",
    "                    time_dif=(X[cols[i]]-X[cols[j]]).astype('timedelta64[D]')\n",
    "                    dif_var_name=cols[i][4:]+'-'+cols[j][4:]\n",
    "                    X[dif_var_name]=time_dif\n",
    "                    cols_derived.append(dif_var_name)\n",
    "            #year, month, day, weekday\n",
    "            for i in cols:\n",
    "                X[i+'year']=X[i].dt.year\n",
    "                X[i+'month']=X[i].dt.month\n",
    "                X[i+'day']=X[i].dt.day\n",
    "                X[i+'weekday']=X[i].dt.weekday\n",
    "                cols_derived.append(i+'year')        \n",
    "                cols_derived.append(i+'month')\n",
    "                cols_derived.append(i+'day') \n",
    "                cols_derived.append(i+'weekday') \n",
    "        #convert the time into days since 1970-01-01\n",
    "        for i in cols:\n",
    "            X[i] = X[i].astype(np.int64)// 10**9 \n",
    "        return X, cols_derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove time columns with 75.0% of nan. There are 6 columns left\n"
     ]
    }
   ],
   "source": [
    "if dropNA_time:\n",
    "    txtotal_nona = dffilter.transform(txtotal,target = 'nan',thresh = dropNAThresh_time)\n",
    "    time_cols_ori = txtotal_nona.columns\n",
    "    print ('Remove time columns with {}% of nan. There are {} columns left'.format(dropNAThresh_time*100, txtotal_nona.shape[1]))\n",
    "else:\n",
    "    txtotal_nona=txtotal.copy()\n",
    "    time_cols_ori = time_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts_derivative_transform = TimeSeriesDerivatives()\n",
    "txtotal_nona_new, time_cols_derived = ts_derivative_transform.transform(txtotal_nona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove outliers\n",
    "#do not apply outlier removal to var_0204, since it is near the end of the month or the beginning of the next month. \n",
    "temp = txtotal_nona_new['VAR_0204day'].copy() \n",
    "txtotal_nona_new, outliers_time_count = remove_outliers_mad(txtotal_nona_new, outlierThresh)\n",
    "txtotal_nona_new['VAR_0204day'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fillin missing values in txtrain\n",
    "# derived variables\n",
    "txtotal_nona_new_d = DataFrameImputer().fit_transform(txtotal_nona_new[time_cols_derived],strategy = fillNAStrategy_time_derived)\n",
    "# original variables\n",
    "txtotal_nona_new_o = DataFrameImputer().fit_transform(txtotal_nona_new[time_cols_ori],strategy = fillNAStrategy_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Separate numerical vs categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataFrameSep(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        separate numerical and categorical feature columns \n",
    "        the transform returns two dataframes, one for each\n",
    "        \"\"\"\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X, y=None,thresh = 10, dropna = True):\n",
    "        \"\"\"\n",
    "        this function separates the numerical and non-numerical columns, output two lists of column names\n",
    "        input:  X, feature dataframe\n",
    "                thresh, number of unique values such that if a column has no more than this number of unique values,\n",
    "                it is treated as categorical even if the dtype is numerical\n",
    "                dropna, if False, nan is counted as an unique value when compare # of unique values with the thresh\n",
    "        output: cX: dataframe of categorical features\n",
    "                nX: dataframe of numerical features\n",
    "\n",
    "        \"\"\"\n",
    "        cat = X.dtypes == 'object'\n",
    "        cat_cols = X.dtypes[cat].index.tolist()\n",
    "        raw_num_cols = X.dtypes[~cat].index.tolist()\n",
    "        num_unique = X[raw_num_cols].apply(lambda x: x.nunique(dropna = dropna),axis = 0)\n",
    "        convert_to_cat_cols = num_unique[num_unique < thresh].index.tolist()\n",
    "        cat_cols.extend(convert_to_cat_cols)\n",
    "        num_cols = [x for x in raw_num_cols if x not in convert_to_cat_cols]\n",
    "        cX,nX = X[cat_cols],X[num_cols]\n",
    "        return cX,nX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sep = DataFrameSep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# non_ts_col: non time series columns\n",
    "non_ts_col = list(set(xtotal.columns).difference(set(time_cols)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cxtotal,nxtotal = sep.transform(xtotal[non_ts_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 259 qualitative features and 1448 quantitative features\n"
     ]
    }
   ],
   "source": [
    "print ('There are {} qualitative features and {} quantitative features'.format(cxtotal.shape[1],nxtotal.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numerical features standardization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two types of columns\n",
    "    1. integer_cols, including columns that have float(NA) and integer values\n",
    "    2. float_cols "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not remove outliers in VAR_0361 (personal income)\n"
     ]
    }
   ],
   "source": [
    "nxtotal, outliers_numeric_count = remove_outliers_mad(nxtotal, thresh=outlierThresh, outlier_uq_thresh=outlierUniqueThresh)\n",
    "if ~remove_personal_income_outlier:\n",
    "    nxtotal['VAR_0361']=xtotal['VAR_0361']#recover personal income\n",
    "    print ('Did not remove outliers in VAR_0361 (personal income)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Derived Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####relocation \n",
    "If current state is different from former state, potentially there was a relocation of this customer. Assumptions: 0237 represents current state name. 0274 represent former state name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# states_current = cxtrain['VAR_0237'].copy()#these two variables will also be used in creating the relocation variables\n",
    "# states_former =  cxtrain['VAR_0274'].copy()#after the log transform etc. \n",
    "##there some typos in the states_former, first get the full list of US states names\n",
    "##an alternative approach is to correct these as DE, GA, TN, and xx, not sure what is RR. Will try it later if had time. \n",
    "##create a list of all the state names, 52 in total, including DC and PR\n",
    "# us_states= list(set(states_former.value_counts().index)-set(['EE', 'GS', 'RN', 'RR'])) \n",
    "# with open('us_states.dat', 'wb') as us_states_outfile:\n",
    "#     pickle.dump(us_states, us_states_outfile)\n",
    "\n",
    "#load in manually checked state names, for checking typos\n",
    "with open('us_states.dat', 'rb') as us_states_infile:\n",
    "    us_states = pickle.load(us_states_infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_typo_state_name(state_names, us_states):\n",
    "    num = len(state_names)\n",
    "    typos = [state_names[i] for i in range(num) if state_names[i] not in us_states]\n",
    "    return len(typos), typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states_current = cxtotal['VAR_0237'].copy()\n",
    "states_former =  cxtotal['VAR_0274'].copy()\n",
    "\n",
    "states_current_names = list(set(states_current.value_counts().index))\n",
    "states_former_names = list(set(states_former.value_counts().index))\n",
    "\n",
    "#check typos by comparing it with the full list of US states.\n",
    "num_typos_current, states_names_typos_c = check_typo_state_name(states_current_names, us_states)\n",
    "num_typos_former, states_names_typos_f = check_typo_state_name(states_former_names, us_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nan != 'CA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#relocation. This variable will join numerical data later\n",
    "missing_former_address = (~states_current.isnull())&states_former.isnull() #former address is missing\n",
    "states_former[missing_former_address]=states_current[missing_former_address]#fill in former with current\n",
    "\n",
    "#even if there are typos in states_former, relocation is still calculated normally.\n",
    "relocation = states_current!=states_former \n",
    "\n",
    "#if current is null, but former is available, assuming no change in address\n",
    "relocation[states_current.isnull()&~(states_former.isnull())] = False\n",
    "\n",
    "#if both states_current and states_former are null, assumming no change in address\n",
    "relocation[states_current.isnull()&states_former.isnull()] = False\n",
    "\n",
    "if sum(relocation.isnull())>0:\n",
    "    #fill any NaN with mode\n",
    "    relocation_mode = relocation.value_counts().index[0]\n",
    "    relocation = relocation.fillna(relocation_mode) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### relative_personal_income, relative_personal_income_binary, relocation_personal_income\n",
    "First, a dictionary of (state name : average income) was created by groupping personal income (0361) based on current state name (0237).\n",
    "\n",
    "relative_personal_income: Generated as personal income (0361) - average income of the current state (0237). \n",
    "\n",
    "relative_personal_income_binary: Generated as relative_personal_income<0. \n",
    "\n",
    "relocation_personal_income: Generated as average personal income of current state - average personal income of former state, \n",
    "possibly reflecting the change in the cost of living due to relocation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# relative_personal_income, relocation_personal_income and relative_personal_income_binary\n",
    "personal_income = xtotal['VAR_0361'].copy()\n",
    "\n",
    "relative_personal_income = pd.Series(index = indices_total, dtype = personal_income.dtypes)#series of NaN\n",
    "relocation_personal_income = pd.Series(index = indices_total, dtype = personal_income.dtypes)#series of NaN\n",
    "\n",
    "state_and_income = pd.concat([states_current, personal_income],axis=1)\n",
    "state_average_income = state_and_income.groupby('VAR_0237').mean()['VAR_0361'].to_dict() #average income in each state\n",
    "\n",
    "state_income_na_indices = state_and_income.isnull().any(axis=1) #either current state or personal income is missing\n",
    "indices_temp = state_and_income[~state_income_na_indices].index\n",
    "\n",
    "for i in indices_temp:\n",
    "    income_temp = personal_income[i]\n",
    "    state_current_temp  = states_current[i]\n",
    "    state_former_temp = states_former[i]\n",
    "    average_income_current_temp = state_average_income.get(state_current_temp, np.nan)\n",
    "    average_income_former_temp = state_average_income.get(state_former_temp, np.nan)\n",
    "    relative_personal_income[i] = income_temp - average_income_current_temp #relative personal income\n",
    "    relocation_personal_income[i] = average_income_current_temp - average_income_former_temp #differences in personal income\n",
    "\n",
    "relative_personal_income_binary = relative_personal_income < 0 #this feature will join numerical data later\n",
    "\n",
    "rpib_mode = relative_personal_income_binary.value_counts().index[0] \n",
    "relative_personal_income_binary = relative_personal_income_binary.fillna(rpib_mode) #fill NA with mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####state_average_income_c, state_average_income_f, state_positive_response_c, state_positive_response_f, state_total_responce_c, state_positive_response_f\n",
    "state_average_income_c: Generated by mapping the current state name (0237) using the (statename: average income) dictionary. \n",
    "\n",
    "state_average_income_f: Generated by mapping the former state name (0274) using the (statename: average income) dictionary. \n",
    "\n",
    "state_positive_response_c: First, a dictionary of  (statename : number of positive responses) was created as the sum of ytrain in each state. Next, a new feature was created by mapping current state name (0237) to number of positive responses of that state. \n",
    "\n",
    "state_positive_response_f: Similar to the feature above, but this feature was created using the former state name (0274) instead.\n",
    "\n",
    "state_total_responce_c: First, a dictionary of (state name : number of occurance in the data) was created. Then a new feature was generated by mapping current state name (0237) to the number of occurance of that state. \n",
    "\n",
    "state_total_responce_f: Similar to the feature above, but this feature was created using the former state name (0274) instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#positive response and total response for each state. \n",
    "#Note that the positive response dictionary is calculated from the training data.\n",
    "response = ytrain.copy()\n",
    "state_and_response_c = pd.concat([states_current.loc[indices_train], response],axis=1)\n",
    "state_and_response_f = pd.concat([states_former.loc[indices_train], response],axis=1)\n",
    "\n",
    "pos_response_c_dict = state_and_response_c.groupby('VAR_0237').sum()['target'].to_dict() #counts of y=1 in each state\n",
    "pos_response_f_dict = state_and_response_f.groupby('VAR_0274').sum()['target'].to_dict() #counts of y=1 in each state\n",
    "\n",
    "tot_response_c_dict = states_current.value_counts().to_dict() #using both training and testing to coutn num of entries\n",
    "tot_response_f_dict = states_former.value_counts().to_dict() #using both training and testing\n",
    "\n",
    "#initialization\n",
    "state_average_income_c = pd.Series(index = indices_total, dtype = personal_income.dtypes)#current state: c\n",
    "state_average_income_f = pd.Series(index = indices_total, dtype = personal_income.dtypes)#former state: f\n",
    "state_positive_response_c = pd.Series(index = indices_total, dtype = response.dtypes)\n",
    "state_positive_response_f = pd.Series(index = indices_total, dtype = response.dtypes)\n",
    "state_total_response_c = pd.Series(index = indices_total, dtype = response.dtypes)\n",
    "state_total_response_f = pd.Series(index = indices_total, dtype = response.dtypes)\n",
    "#using current state\n",
    "indices_states_c = states_current[states_current.notnull()].index\n",
    "for i in indices_states_c:\n",
    "    state_temp = states_current[i]\n",
    "    if state_temp in us_states: #excluding any typos\n",
    "        state_average_income_c[i] = state_average_income.get(state_temp, np.nan)\n",
    "        state_positive_response_c[i] = pos_response_c_dict.get(state_temp, np.nan)\n",
    "        state_total_response_c[i] = tot_response_c_dict.get(state_temp, np.nan)\n",
    "#using former state\n",
    "indices_states_f = states_former[states_former.notnull()].index\n",
    "for i in indices_states_f:\n",
    "    state_temp = states_former[i]\n",
    "    if state_temp in us_states: #excluding any typos\n",
    "        state_average_income_f[i] = state_average_income.get(state_temp, np.nan)\n",
    "        state_positive_response_f[i] = pos_response_f_dict.get(state_temp, np.nan)\n",
    "        state_total_response_f[i] = tot_response_f_dict.get(state_temp, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Adding the derived features to nxtotal\n",
    "There are 10 derived features in total. Except for \"relocation\" and \"relative_personal_income_binary\" which are binary, all variables will be processed as numerical features. The two binary features will join the numerical features, after the normalization procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nxtotal['relative_personal_income'] = relative_personal_income\n",
    "nxtotal['relocation_personal_income'] = relocation_personal_income\n",
    "nxtotal['state_average_income_c'] = state_average_income_c\n",
    "nxtotal['state_average_income_f'] = state_average_income_f\n",
    "nxtotal['state_positive_response_c'] = state_positive_response_c\n",
    "nxtotal['state_positive_response_f'] = state_positive_response_f\n",
    "nxtotal['state_total_response_c'] = state_total_response_c\n",
    "nxtotal['state_total_response_f'] = state_total_response_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift feature values to positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ShiftPostive(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        shift selected numerical columns to positive value\n",
    "        \"\"\"\n",
    "        \n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def test_int(self,colvector):\n",
    "        \"\"\"\n",
    "        test if the float dtype columns have no non-integer values other than NA\n",
    "        input: column vector\n",
    "        output: boolean, True if the column has no non-integer value other than NA\n",
    "        \"\"\"\n",
    "        return colvector[colvector.notnull()].apply(lambda x:x.is_integer()).sum() == len(colvector[colvector.notnull()])\n",
    "\n",
    "    def transform(self,X,y=None, int_amount = 1, deci_amount = 0.1):\n",
    "        # separate out integer vs float-valued columns\n",
    "        int_cols = X.dtypes[X.dtypes == np.dtype('int64')].index.tolist()\n",
    "        float_cols = X.dtypes[X.dtypes == np.dtype('float64')].index.tolist()\n",
    "        int_with_nans_bool = X[float_cols].apply(self.test_int)\n",
    "        int_with_nans = int_with_nans_bool[int_with_nans_bool].index.tolist()\n",
    "        int_cols.extend(int_with_nans)\n",
    "        float_cols = list(set(float_cols).difference(set(int_with_nans)))\n",
    "        # shift integer-valued columns and float valued columns separately\n",
    "        new_int_df = X[int_cols].apply(lambda x: x - x.min() + int_amount if x.min() <= 0 else x)\n",
    "        new_float_df = X[float_cols].apply(lambda x: x - x.min() + deci_amount if x.min() <= 0 else x)\n",
    "        return pd.merge(new_int_df,new_float_df,left_index=True,right_index=True),int_cols,float_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shiftpos = ShiftPostive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nxtotal_shifted,int_cols,float_cols = shiftpos.transform(nxtotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#release memory\n",
    "%xdel nxtotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few columns in float_cols have 99.0 as maximum, 99.0 is suspected to be NAs, may replace with nan later to test; Also, all float_cols are non-negative with minimum equals zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Log transform to highly right-skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogTransform(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        perform log transformation to columns that are uni-modal and right skewed\n",
    "        \"\"\"\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def normaltest(self,colvec,test = 'normal'):\n",
    "        \"\"\"\n",
    "        test if a column feature has normal distribution using the stats.mstats.normaltest, skewtest, or kurtosistest\n",
    "        notably, strong multi-modal data will have a masked value returned from kurtosis test, therefore can be filtered\n",
    "        after this function is called\n",
    "        input: colvec, column vector, in the format of pandas series\n",
    "                test: 'normal', 'skew','kurtosis'\n",
    "        output: \n",
    "            for normal:\n",
    "            k^2 + s^2, where k and s are the Z-score returned by the kurtosis test and the skew test\n",
    "            for a perfect normal distribution, k is 3 and s is zero\n",
    "            for skew or kurtosis:\n",
    "            z-score \n",
    "        \"\"\"\n",
    "        if test == 'normal':\n",
    "            return stats.mstats.normaltest(colvec[colvec.notnull()])[0]\n",
    "        elif test == 'skew':\n",
    "            return stats.mstats.skewtest(colvec[colvec.notnull()])[0]\n",
    "        elif test == 'kurtosis':\n",
    "            return stats.mstats.kurtosistest(colvec[colvec.notnull()])[0]\n",
    "        else:\n",
    "            print ('unknown test type')\n",
    "            return\n",
    "    \n",
    "    def transform(self,X,y=None,thresh = 5000):\n",
    "        #1. apply normal test and determine cols to transform\n",
    "        test_results = X.apply(self.normaltest)\n",
    "        multi_modal_cols = test_results[test_results.apply(lambda x: x is ma.masked)].index.tolist()\n",
    "        to_transform_cols = test_results[test_results > thresh].index.tolist()\n",
    "        #2. perform log transform\n",
    "        transformed_cols = X[to_transform_cols].apply(lambda x: np.log(x) if test_results[x.name] > thresh else x)\n",
    "        cols = X.columns.tolist()\n",
    "        unchanged_cols = list(set(cols).difference(set(to_transform_cols)))\n",
    "        return pd.merge(X[unchanged_cols],transformed_cols,right_index = True,left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logtrans = LogTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nxtotal_log = logtrans.transform(nxtotal_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'nxtrain_shifted' is not defined\n"
     ]
    }
   ],
   "source": [
    "#release memory\n",
    "%xdel nxtrain_shifted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Standardize numerical data using standardscaler, first need to impute "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us no choice but impute some reasonable values e.g., median, mode, mean if we want to scale the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imputer = DataFrameImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nxtotal_imputed = imputer.fit_transform(nxtotal_log,strategy = 'median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'nxtrain_log #release memory' is not defined\n"
     ]
    }
   ],
   "source": [
    "%xdel nxtrain_log #release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nxtotal_standard = scaler.fit_transform(nxtotal_imputed.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#release memory\n",
    "%xdel nxtotal_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290463, 1456)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxtotal_standard.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Add relocation and relative_personal_income_binary to the end of the numerical features. \n",
    "#####The last 10 columns in numerical features correspond to the derived features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rows, cols = nxtotal_standard.shape\n",
    "nxtotal_standard_new = np.zeros((rows, cols+2))\n",
    "nxtotal_standard_new[:,:-2] = nxtotal_standard\n",
    "nxtotal_standard_new[:,-2] = relative_personal_income_binary.values\n",
    "nxtotal_standard_new[:,-1] = relocation.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#release memory\n",
    "%xdel nxtotal_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290463, 1458)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxtotal_standard_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Some extra functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a plot hist function using log scale\n",
    "def counthist(feature_name, data, ax):\n",
    "    \"\"\"\n",
    "    data is in DataFrame format. \n",
    "    the feature column must be non-negative\n",
    "    0.1 is added to the feature column such that the column is positive\n",
    "    inputs: feature_name, the column name, ax, the current axis\n",
    "    outputs: a hist plot that has x axis on log scale\n",
    "    \"\"\"\n",
    "    feature = data[feature_name] + 0.1\n",
    "    start = np.log10(feature.min());stop = np.log10(feature.max())\n",
    "    ax.set_xscale('log')\n",
    "    feature.hist(bins = np.logspace(start,stop,30),ax = ax,label = feature_name)\n",
    "    ax.legend(loc = 'best',fontsize = 'medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Categorical features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### categorical columns divide into two types, obj-type columns and ordinal(numerical)-type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290463, 259)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cxtotal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def consolidate(strings_input):\n",
    "    \"\"\"\n",
    "    Transform in-place the given dataset by consolidating\n",
    "    rare categorical features into a single category.\n",
    "    \"\"\"\n",
    "    strings_consolidate = strings_input.copy()\n",
    "    strings_value_counts =  strings_input.value_counts()\n",
    "    rare_strings = strings_value_counts[strings_value_counts==1].index\n",
    "    for i in rare_strings:\n",
    "        strings_consolidate[strings_input==i]='rare_string'\n",
    "    return strings_consolidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####First, object-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obj_cols = cxtotal.dtypes[cxtotal.dtypes == 'object'].index.tolist()\n",
    "num_cat_cols = cxtotal.dtypes[cxtotal.dtypes != 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if dropNA_cat:\n",
    "    cxtotal_obj_nona = dffilter.transform(cxtotal[obj_cols],target = 'nan',thresh = dropNAThresh_cat)\n",
    "    print ('Remove text columns with {}% of nan. There are {} columns left'.format(dropNAThresh_cat*100, cxtrain_obj_nona.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CatVectorize(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        wrapper to use DictVectorizer to transform obj-type categorical features\n",
    "        \"\"\"\n",
    "    def fit(self,X,y=None):\n",
    "        return\n",
    "    \n",
    "    def cat_vectorize(self,X):\n",
    "        \"\"\"\n",
    "        vectorize the categorical features of the input dataframe\n",
    "        take the two columns VAR_0001, and VAR_0005 for example,\n",
    "            cat_vectorize(cxtrain,obj_cols[:2])\n",
    "            for each feature, each value is used to construct a binary feature, since there are 7 values in total\n",
    "            for the two features combined, the transformed matrix has seven columns, corresponding column values are\n",
    "            stored in encoder.feature_names\n",
    "\n",
    "        Notably, NA is encoded as a value by filling NAs with 'NA' first\n",
    "\n",
    "        inputs: df, dataframe; cols, list of columns to be transformed\n",
    "        output: data_mt, encoded sparse matrix, can be converted to array with \"toarray\" method\n",
    "                encoder.feature_names, explained above\n",
    "        \"\"\"\n",
    "        data = X.fillna('NA')\n",
    "        datadict = data.T.to_dict().values()\n",
    "        encoder = DictVectorizer()\n",
    "        data_mt = encoder.fit_transform(datadict)\n",
    "        return encoder.feature_names_,data_mt\n",
    "    \n",
    "    def transform(self,X,y=None,thresh=None):\n",
    "        \"\"\"\n",
    "        inputs\n",
    "            thresh: is the maximum num of values a column can have to be qualified for the transformation\n",
    "            columns that have more than thresh number of values are likely states, or description that may need NLP\n",
    "            X: df of categorical features\n",
    "        outputs:\n",
    "            X_vect, the sparse matrix contains only the converted features\n",
    "            other_cols, list of the other obj-type columns that have more than thresh number of values\n",
    "            feature_names, the corresponding feature values used as columns in X_vect\n",
    "        \"\"\"\n",
    "        if thresh:\n",
    "            value_counts = X.apply(lambda x: x.nunique(dropna = False))\n",
    "            obj_cols = value_counts[value_counts < thresh].index.tolist()\n",
    "            other_cols = list(set(X.columns).difference(set(obj_cols)))\n",
    "            X_to_trans = X[obj_cols]\n",
    "        else:\n",
    "            X_to_trans = X.copy()\n",
    "        feature_names,X_vect = self.cat_vectorize(X_to_trans)   \n",
    "        return X_vect,feature_names,other_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16582\n",
      "9618\n"
     ]
    }
   ],
   "source": [
    "#consolidate VAR_0200: city names\n",
    "print (len(cxtotal['VAR_0200'].value_counts()))\n",
    "temp1 = cxtotal['VAR_0200'].copy()    \n",
    "temp2 = consolidate(temp1)\n",
    "cxtotal['VAR_0200'] = temp2 \n",
    "print (len(cxtotal['VAR_0200'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CatVectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if dropNA_cat:\n",
    "    obj_sparse,feature_names,other_cols = vect.transform(cxtotal_obj_nona,thresh = 60)\n",
    "else:\n",
    "    obj_sparse,feature_names,other_cols = vect.transform(cxtotal[obj_cols],thresh = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(other_cols)>0:\n",
    "    #for other_cols, apply simple label encoding\n",
    "    imputer_cat = DataFrameImputer()\n",
    "    cxtotal_others_imputed = cxtotal[other_cols].fillna('NA') #fill NaN with NA\n",
    "    le = LabelEncoder()\n",
    "    for c in cxtotal_others_imputed.columns:\n",
    "        cxtotal_others_imputed[c]=le.fit_transform(cxtotal_others_imputed[c].astype(str))\n",
    "    print ('Label encoding the other columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#categorical features with numerical data type, simply fillna with mode. No additional transformation.\n",
    "imputer = DataFrameImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cxtotal_num_imputed = imputer.fit_transform(cxtotal[num_cat_cols],strategy = fillNAStrategy_cat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(other_cols)>0:\n",
    "    #combine cxtotal_num_imputed and cxtotal_others_imputed\n",
    "    cxtotal_num_imputed = pd.concat([cxtotal_num_imputed, cxtotal_others_imputed], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####For numerical columns with fewer than threshold (e.g., 10, as a result of previous filtering thresh) numerical values, conversion to categorical will lead to lost of ordinal info; care must be taken; However, if we want to transform, here are the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ordinal_col = cxtrain.dtypes[cxtrain.dtypes != 'object'].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OrdiVectorize(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        vectorize ordinal columns, namely numerical columns with fewer than the previous thresh num of values\n",
    "        see DataFrameSep for details\n",
    "        \"\"\"\n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def ordinal_vectorize(self,X,y=None,strategy = -9999):\n",
    "        \"\"\"\n",
    "        vectorize ordinal columns, first need to label_encode such that discrete values are converted to \n",
    "        continuous integer set from 0 to num_values - 1, \n",
    "\n",
    "        second, use one hot encoder to convert each value into binary \n",
    "\n",
    "        inputs: X, dataframe, strategy, value used to fillna\n",
    "        outputs: sparse matrix of the transformed feature matrix\n",
    "        \"\"\"\n",
    "        data = X.fillna(strategy)\n",
    "        le = LabelEncoder()\n",
    "        hot = OneHotEncoder()\n",
    "        for col in data.columns:\n",
    "            data[col] = le.fit_transform(np.array(data[col]))\n",
    "        # second step: convert each value for each feature into a binary feature using onehot encoder\n",
    "        # ordinal information is lost \n",
    "        data_mt = hot.fit_transform(data.as_matrix())\n",
    "        return data_mt\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        \"\"\"\n",
    "        return self.ordinal_vectorize(X)\n",
    "        \"\"\"\n",
    "        return self.ordinal_vectorize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vect2 = OrdiVectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ordi_sparse = vect2.transform(cxtrain[ordinal_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ordi_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Save processed data to disk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Summary of processed data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:\n",
    "1. nxtrain_standard_original0: normalized original numerical features. \n",
    "2. nxtrain_standard_derived0:  normalized derived numerical features. \n",
    "3. cat_numeric_th60_train2: categorical features with numerical types. No transformation was applied at the moment. The \"2\" in the end of the file name indicates the protocol used in pickle.dump.\n",
    "4. cat_sparse_th60_train2: one-hot encoded obj-dtype categorical features, sparse matrix. \n",
    "5. time_series_original_train2: original time series. Those with more than 75% NAs are removed. \n",
    "6. time_series_derived_train2: derived time series. \n",
    "\n",
    "Test data:\n",
    "1. nxtest_standard_original0.\n",
    "2. nxtest_standard_derived0.\n",
    "3. cat_numeric_th60_test2.\n",
    "4. cat_sparse_th60_test2.\n",
    "5. time_series_original_test2.\n",
    "6. time_series_derived_test2\n",
    "\n",
    "Target:\n",
    "1. ytrain2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1. Save all numeric data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separating original and derived numerical features (10 derived features in total)\n",
    "#nxtrain\n",
    "nxtrain_standard_original = nxtotal_standard_new[:len(indices_train), :-10]\n",
    "nxtrain_standard_derived  = nxtotal_standard_new[:len(indices_train), -10:]\n",
    "#nxtest\n",
    "nxtest_standard_original = nxtotal_standard_new[len(indices_train):, :-10]\n",
    "nxtest_standard_derived  = nxtotal_standard_new[len(indices_train):, -10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.save('nxtrain_standard_original0',nxtrain_standard_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('nxtrain_standard_derived0',nxtrain_standard_derived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('nxtest_standard_original0',nxtest_standard_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('nxtest_standard_derived0',nxtest_standard_derived)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Save all processed categorical data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separate categorical features in numeric dtypes for cxtrain and cxtest\n",
    "cxtrain_num = cxtotal_num_imputed.iloc[:len(indices_train),:]\n",
    "cxtest_num = cxtotal_num_imputed.iloc[len(indices_train):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145231, 13769)\n",
      "(145232, 13769)\n"
     ]
    }
   ],
   "source": [
    "#separate train and test sparse matrices\n",
    "obj_sparse_train = obj_sparse[:len(indices_train), :]\n",
    "obj_sparse_test = obj_sparse[len(indices_train):, :]\n",
    "print (obj_sparse_train.shape)\n",
    "print (obj_sparse_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cat_numeric_th60_train2.dat', 'wb') as cat_outfile1:\n",
    "    pickle.dump(cxtrain_num, cat_outfile1, protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cat_sparse_th60_train2.dat', 'wb') as cat_outfile2:\n",
    "    pickle.dump(obj_sparse_train, cat_outfile2, protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cat_numeric_th60_test2.dat', 'wb') as cat_outfile3:\n",
    "    pickle.dump(cxtest_num, cat_outfile3, protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('cat_sparse_th60_test2.dat', 'wb') as cat_outfile4:\n",
    "    pickle.dump(obj_sparse_test, cat_outfile4, protocol =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "####3. Save time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#separate original and derived time series for train and test\n",
    "# train\n",
    "txtrain_original= txtotal_nona_new_o.iloc[:len(indices_train),:]\n",
    "txtrain_derived = txtotal_nona_new_d.iloc[:len(indices_train),:]\n",
    "#test\n",
    "txtest_original= txtotal_nona_new_o.iloc[len(indices_train):,:]\n",
    "txtest_derived = txtotal_nona_new_d.iloc[len(indices_train):,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('time_series_original_train2.dat', 'wb') as time_outfile1:\n",
    "    pickle.dump(txtrain_original, time_outfile1, protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('time_series_derived_train2.dat', 'wb') as time_outfile2:\n",
    "    pickle.dump(txtrain_derived, time_outfile2, protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('time_series_original_test2.dat', 'wb') as time_outfile3:\n",
    "    pickle.dump(txtest_original, time_outfile3, protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('time_series_derived_test2.dat', 'wb') as time_outfile4:\n",
    "    pickle.dump(txtest_derived, time_outfile4, protocol =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####4. Save target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('ytrain2.dat', 'wb') as target_outfile2:\n",
    "    pickle.dump(ytrain, target_outfile2, protocol =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
